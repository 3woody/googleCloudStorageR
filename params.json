{
  "name": "Googlecloudstorager",
  "tagline": "Google Cloud Storage API to R",
  "body": "# googleCloudStorageR\r\n\r\nR library for interacting with the Google Cloud Storage JSON API ([api docs](https://cloud.google.com/storage/docs/json_api/)).\r\n\r\n## Setup\r\n\r\nGoogle Cloud Storage charges you for storage [(prices here)](https://cloud.google.com/storage/pricing).\r\n\r\nYou can use your own Google Project with a credit card added to create buckets, where the charges will apply.  This can be done in the [Google API Console](https://console.developers.google.com)\r\n\r\n### Setting environment variables\r\n\r\nBy default, all cloudyr packages look for the access key ID and secret access key in environment variables. You can also use this to specify a default bucket, and auto-authentication upon attaching the library. For example:\r\n\r\n```r\r\nSys.setenv(\"GCS_CLIENT_ID\" = \"mykey\",\r\n           \"GCS_CLIENT_SECRET\" = \"mysecretkey\",\r\n           \"GCS_WEB_CLIENT_ID\" = \"my-shiny-key\",\r\n           \"GCS_WEB_CLIENT_SECRET\" = \"my-shiny-secret-key\",\r\n           \"GCS_DEFAULT_BUCKET\" = \"my-default-bucket\",\r\n           \"GCS_AUTH_FILE\" = \"/fullpath/to/service-auth.json\")\r\n```\r\n\r\nThese can alternatively be set on the command line or via an Renviron.site or .Renviron file ([see here for instructions](https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html)).\r\n\r\n## Authentication\r\n\r\nAuthentication can be carried out each session via `gcs_auth`.  The first time you run this you will be sent to a Google login prompt in your browser to allow the `googleCloudStorageR` project access (or the Google project you configure). \r\n\r\nOnce authenticated a file named `.httr-oauth` is saved to your working directory.  On subsequent authentication this file will hold your authentication details, and you won't need to go via the browser.  Deleting this file, or setting `new_user=TRUE` will start the authentication flow again.\r\n\r\n```r\r\nlibrary(googleCloudStorageR)\r\n## first time this will send you to the browser to authenticate\r\ngcs_auth()\r\n\r\n## to authenticate with a fresh user, delete .httr-oauth or run with new_user=TRUE\r\ngcs_auth(new_user = TRUE)\r\n\r\n...call functions...etc...\r\n\r\n```\r\n\r\nEach new R session will need to run `gcs_auth()` to authenticate future API calls.\r\n\r\n### Auto-authentication\r\n\r\nAlternatively, you can specify the location of a service account JSON file taken from your Google Project, or the location of a previously created `.httr-oauth` token in a system environment:\r\n\r\n        Sys.setenv(\"GCS_AUTH_FILE\" = \"/fullpath/to/auth.json\")\r\n\r\nThis file will then used for authentication via `gcs_auth()` when you load the library:\r\n\r\n```r\r\n## GCS_AUTH_FILE set so auto-authentication\r\nlibrary(googleCloudStorageR)\r\n\r\n## no need for gcs_auth()\r\ngcs_get_bucket(\"your-bucket\")\r\n\r\n```\r\n\r\n## Examples\r\n\r\n### Setting a default Bucket\r\n\r\nTo avoid specifying the bucket in the functions below, you can set the name of your default bucket via environmental variables or via the function `gcs_global_bucket()`.  See the `Setting environment variables` section below for more details.\r\n\r\n```r\r\n## set bucket via environment\r\nSys.setenv(\"GCS_DEFAULT_BUCKET\" = \"my-default-bucket\")\r\n\r\nlibrary(googleCloudStorageR)\r\n\r\n## optional, if you haven't set environment argument GCS_AUTH_FILE\r\n## gcs_auth()\r\n\r\n## check what the default bucket is\r\ngcs_get_global_bucket()\r\n[1] \"my-default-bucket\"\r\n\r\n## you can also set a default bucket after loading the library for that session\r\ngcs_global_bucket(\"your-default-bucket-2\")\r\ngcs_get_global_bucket()\r\n[1] \"my-default-bucket-2\"\r\n```\r\n\r\n### Downloading objects from Google Cloud storage\r\n\r\nOnce you have a Google project and created a bucket with an object in it, you can download it as below:\r\n\r\n```r\r\nlibrary(googleCloudStorageR)\r\n\r\n## optional, if you haven't set environment argument GCS_AUTH_FILE\r\n## gcs_auth()\r\n\r\n## get your project name from the API console\r\nproj <- \"your-project\"\r\n\r\n## get bucket info\r\nbuckets <- gcs_list_buckets(proj)\r\nbucket <- \"your-bucket\"\r\nbucket_info <- gcs_get_bucket(bucket)\r\nbucket_info\r\n\r\n==Google Cloud Storage Bucket==\r\nBucket:          your-bucket \r\nProject Number:  1123123123 \r\nLocation:        EU \r\nClass:           STANDARD \r\nCreated:         2016-04-28 11:39:06 \r\nUpdated:         2016-04-28 11:39:06 \r\nMeta-generation: 1 \r\neTag:            Cxx=\r\n\r\n\r\n## get object info in the default bucket\r\nobjects <- gcs_list_objects()\r\n\r\n## save directly to an R object (warning, don't run out of RAM if its a big object)\r\n## the download type is guessed into an appropriate R object\r\nparsed_download <- gcs_get_object(objects$name[[1]])\r\n\r\n## if you want to do your own parsing, set parseObject to FALSE\r\n## use httr::content() to parse afterwards\r\nraw_download <- gcs_get_object(objects$name[[1]], \r\n                               parseObject = FALSE)\r\n\r\n## save directly to a file in your working directory\r\n## parseObject has no effect, it is a httr::content(req, \"raw\") download\r\ngcs_get_object(objects$name[[1]], saveToDisk = \"csv_downloaded.csv\")\r\n```\r\n\r\n## Uploading objects < 5MB\r\n\r\nObjects can be uploaded via files saved to disk, or passed in directly if they are data frames or list type R objects.  By default, data frames will be converted to CSV via `write.csv()`, lists to JSON via `jsonlite::toJSON`.\r\n\r\nIf you want to use other functions for transforming R objects, for example setting `row.names = FALSE` or using `write.csv2`, pass the function through `object_function`\r\n\r\n```r\r\n## upload a file - type will be guessed from file extension or supply type  \r\nwrite.csv(mtcars, file = filename)\r\ngcs_upload(filename)\r\n\r\n## upload an R data.frame directly - will be converted to csv via write.csv\r\ngcs_upload(mtcars)\r\n\r\n## upload an R list - will be converted to json via jsonlite::toJSON\r\ngcs_upload(list(a = 1, b = 3, c = list(d = 2, e = 5)))\r\n\r\n## upload an R data.frame directly, with a custom function\r\n## function should have arguments 'input' and 'output'\r\n## safest to supply type too\r\nf <- function(input, output) write.csv(input, row.names = FALSE, file = output)\r\n\r\ngcs_upload(mtcars, \r\n           object_function = f,\r\n           type = \"text/csv\")\r\n```\r\n\r\n## Upload metadata\r\n\r\nYou can pass metadata with an object via the function `gcs_metadata_object()`.\r\n\r\nthe name you pass to the metadata object will override the name if it is also set elsewhere.\r\n\r\n```r\r\nmeta <- gcs_metadata_object(\"mtcars.csv\",\r\n                             metadata = list(custom1 = 2,\r\n                                             custom_key = 'dfsdfsdfsfs))\r\n                                             \r\ngcs_upload(mtcars, object_metadata = meta)\r\n```\r\n\r\n\r\n## Resumable uploads for files > 5MB up to 5TB\r\n\r\nIf the file/object is under 5MB, simple uploads are used.  \r\n\r\nFor files > 5MB, [resumable uploads](https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload#resumable) are used.  This allows you to upload up to 5TB.  \r\n\r\nIf you get an interrupted connection when uploading, `gcs_upload` will retry 3 times, if it fails it will return a Retry object, that you can try again later from where the upload stopped.  Call this via `gcs_retry_upload`\r\n\r\n```r\r\n## write a big object to a file\r\nbig_file <- \"big_filename.csv\"\r\nwrite.csv(big_object, file = big_file)\r\n\r\n## attempt upload\r\nupload_try <- gcs_upload(big_file)\r\n\r\n## if successful, upload_try is an object metadata object\r\nupload_try\r\n==Google Cloud Storage Object==\r\nName:            \"big_filename.csv\" \r\nSize:            8.5 Gb \r\nMedia URL        https://www.googleapis.com/download/storage/v1/b/xxxx \r\nBucket:          your-bucket \r\nID:              your-bucket/\"test.pdf\"/xxxx\r\nMD5 Hash:        rshao1nxxxxxY68JZQ== \r\nClass:           STANDARD \r\nCreated:         2016-08-12 17:33:05 \r\nUpdated:         2016-08-12 17:33:05 \r\nGeneration:      1471023185977000 \r\nMeta Generation: 1 \r\neTag:            CKi90xxxxxEAE= \r\ncrc32c:          j4i1sQ== \r\n\r\n\r\n## if unsuccessful after 3 retries, upload_try is a Retry object\r\n==Google Cloud Storage Upload Retry Object==\r\nFile Location:     big_filename.csv\r\nRetry Upload URL:  http://xxxx\r\nCreated:           2016-08-12 17:33:05 \r\nType:              csv\r\nFile Size:        8.5 Gb\r\nUpload Byte:      4343\r\nUpload remaining: 8.1 Gb\r\n\r\n## you can retry to upload the remaining data using gcs_retry_upload()\r\ntry2 <- gcs_retry_upload(upload_try)\r\n```\r\n\r\n## Updating user access to objects\r\n\r\nYou can change who can access objects via `gcs_update_acl` to one of `READER` or `OWNER`, on a user, group, domain, project or public for all users or authenticated users. \r\n\r\nBy default you are \"OWNER\" of all the objects and buckets you upload and create.\r\n\r\n```r\r\n## update access of object to READER for all public\r\ngcs_update_acl(\"your-object.csv\", entity_type = \"allUsers\")\r\n\r\n## update access of object for user joe@blogs.com to OWNER\r\ngcs_update_acl(\"your-object.csv\", \r\n               entity = \"joe@blogs.com\", \r\n               role = \"OWNER\")\r\n\r\n## update access of object for googlegroup users to READER\r\ngcs_update_acl(\"your-object.csv\", \r\n               entity = \"my-group@googlegroups.com\", \r\n               entity_type = \"group\")\r\n\r\n## update access of object for all users to OWNER on your Google Apps domain\r\ngcs_update_acl(\"your-object.csv\", \r\n               entity = \"yourdomain.com\", \r\n               entity_type = \"domain\", \r\n               role = OWNER)\r\n```\r\n\r\n## Deleting an object\r\n\r\nDelete an object by passing its name (and bucket if not default)\r\n\r\n```r\r\n## returns TRUE is successful, a 404 error if not found\r\ngcs_delete_object(\"your-object.csv\")\r\n```\r\n\r\n### Viewing current access level to objects\r\n\r\nUse `gcs_get_object_access()` to see what the current access is for an `entity` + `entity_type`.\r\n\r\n```r\r\n## default entity_type is user\r\nacl <- gcs_get_object_access(\"your-object.csv\", \r\n                             entity = \"joe@blogs.com\")\r\nacl$role \r\n[1] \"OWNER\"\r\n\r\n## for allUsers and allAuthenticated users, you don't need to supply entity\r\nacl <- gcs_get_object_access(\"your-object.csv\", \r\n                             entity_type = \"allUsers\")\r\nacl$role \r\n[1] \"READER\"\r\n```\r\n\r\n### Creating download links\r\n\r\nOnce a user (or group or the public) has access, they can reach that object via a download link generated by the function `gcs_download_url`\r\n\r\n```r\r\ndownload_url <- gcs_download_url(\"your-object.csv\")\r\ndownload_url\r\n[1] \"https://storage.cloud.google.com/your-project/your-object.csv\"\r\n```\r\n\r\n## R Session helpers\r\n\r\nVersions of `save.image()` and `load()` are implemented called `gcs_save()` and `gcs_load()`.  These functions save and load the global R session to the cloud.\r\n\r\n```r\r\n## save the current R session including all objects\r\ngcs_save()\r\n\r\n### wipe environment\r\nrm(list = ls())\r\n\r\n## load up environment again\r\ngcs_load()\r\n```\r\n\r\nYou can also upload `.R` code files and source them directly using `gcs_source`:\r\n\r\n```r\r\n## make a R source file and upload it\r\ncat(\"x <- 'hello world!'\\nx\", file = \"example.R\")\r\ngcs_upload(\"example.R\", name = \"example.R\")\r\n\r\n## source the file to run its code\r\ngcs_source(\"example.R\")\r\n\r\n## the code from the upload file has run\r\nx\r\n[1] \"hello world!\"\r\n```\r\n\r\n## Uploading via a Shiny app\r\n\r\nThe library is also compatible with Shiny authentication flows, so you can create Shiny apps that lets users log in and upload their own data.  \r\n\r\nAn example of that is shown below:\r\n\r\n```r\r\nlibrary(\"shiny\")\r\nlibrary(\"googleAuthR\")\r\nlibrary(\"googleCloudStorageR\")\r\noptions(googleAuthR.scopes.selected = \"https://www.googleapis.com/auth/devstorage.full_control\")\r\n## optional, if you want to use your own Google project\r\n# options(\"googleAuthR.client_id\" = \"YOUR_CLIENT_ID\")\r\n# options(\"googleAuthR.client_secret\" = \"YOUR_CLIENT_SECRET\")\r\n\r\n## you need to start Shiny app on port 1221\r\n## as thats what the default googleAuthR project expects for OAuth2 authentication\r\n\r\n## options(shiny.port = 1221)\r\n## print(source('shiny_test.R')$value) or push the \"Run App\" button in RStudio\r\n\r\nshinyApp(\r\n  ui = shinyUI(\r\n      fluidPage(\r\n        googleAuthR::googleAuthUI(\"login\"),\r\n        fileInput(\"picture\", \"picture\"),\r\n        textInput(\"filename\", label = \"Name on Google Cloud Storage\",value = \"myObject\"),\r\n        actionButton(\"submit\", \"submit\"),\r\n        textOutput(\"meta_file\")\r\n      )\r\n  ),\r\n  server = shinyServer(function(input, output, session){\r\n\r\n    access_token <- shiny::callModule(googleAuth, \"login\")\r\n\r\n    meta <- eventReactive(input$submit, {\r\n\r\n      message(\"Uploading to Google Cloud Storage\")\r\n      \r\n      # from googleCloudStorageR\r\n      with_shiny(gcs_upload,  \r\n                 file = input$picture$datapath,\r\n                 # enter your bucket name here\r\n                 bucket = \"gogauth-test\",  \r\n                 type = input$picture$type,\r\n                 name = input$filename,\r\n                 shiny_access_token = access_token())\r\n\r\n    })\r\n\r\n    output$meta_file <- renderText({\r\n      \r\n      req(meta())\r\n\r\n      str(meta())\r\n\r\n      paste(\"Uploaded: \", meta()$name)\r\n\r\n    })\r\n\r\n  })\r\n)\r\n```\r\n\r\n## Bucket administration\r\n\r\nThere are various functions to manipulate Buckets:\r\n\r\n* `gcs_list_buckets`\r\n* `gcs_get_bucket`\r\n* `gcs_create_bucket`\r\n* `gcs_update_bucket`\r\n\r\n## Object administration\r\n\r\nYou can get meta data about an object by passing `meta=TRUE` to `gcs_get_object`\r\n\r\n```r\r\ngcs_get_object(\"your-object\", \"your-bucket\", meta = TRUE)\r\n```\r\n\r\n## Explanation of Google Project access\r\n\r\n`googleCloudStorageR` has its own Google project which is used to call the Google Cloud Storage API, but does not have access to the objects or buckets in your Google Project unless you give permission for the library to access your own buckets during the OAuth2 authentication process.  \r\n\r\nNo other user, including the owner of the Google Cloud Storage API project has access unless you have given them access, but you may want to change to use your own Google Project (that could or could not be the same as the one that holds your buckets).  \r\n\r\n## Configuring your own Google Project\r\n\r\nThe instructions below are for when you visit the Google API console (`https://console.developers.google.com/apis/`)\r\n\r\n### For local use\r\n\r\n1. Click 'Create a new Client ID', and choose \"Installed Application\".\r\n2. Note your Client ID and secret.\r\n3. Add them by modifying your .Renviron file, or under the following entries:\r\n\r\n        Sys.setenv(\"GCS_CLIENT_ID\" = \"mykey\",\r\n                   \"GCS_CLIENT_SECRET\" = \"mysecretkey\")\r\n\r\n4. Alternatively, modify these options after googleAuthR has been loaded:\r\n\r\n        options(\"googleAuthR.client_id\" = \"YOUR_CLIENT_ID\")\r\n        options(\"googleAuthR.client_secret\" = \"YOUR_CLIENT_SECRET\")\r\n\r\n### For Shiny use\r\n\r\n1. Click 'Create a new Client ID', and choose \"Web Application\".\r\n2. Note your Client ID and secret.\r\n3. Add the URL of where your Shiny app will run, with no port number. e.g. `https://mark.shinyapps.io/searchConsoleRDemo/`\r\n4. And/Or also put in localhost or 127.0.0.1 with a port number for local testing. Remember the port number you use as you will need it later to launch the app e.g. `http://127.0.0.1:1221`\r\n5. Add them by modifying your .Renviron file, or under the following entries:\r\n\r\n        Sys.setenv(\"GCS_WEB_CLIENT_ID\" = \"mykey\",\r\n                   \"GCS_WEB_CLIENT_SECRET\" = \"mysecretkey\")\r\n                   \r\n6. Alternatively, in your Shiny script modify these options:\r\n\r\n        options(\"googleAuthR.webapp.client_id\" = \"YOUR_CLIENT_ID\")\r\n        options(\"googleAuthR.webapp.client_secret\" = \"YOUR_CLIENT_SECRET\")\r\n\r\n7. To run the app locally specifying the port number you used in step 4 e.g. `shiny::runApp(port=1221)` or set a shiny option to default to it: `options(shiny.port = 1221)` and launch via the `RunApp` button in RStudio.\r\n8. Running on your Shiny Server will work only for the URL from step 3.\r\n\r\n### Activate API\r\n\r\n1. Click on \"APIs\"\r\n2. Select and activate the Cloud Storage JSON API \r\n3. After loading the package via `library(googleCloudStorage)`, it will look to see if `\"https://www.googleapis.com/auth/devstorage.full_control\"` is set in `getOption(\"googleAuthR.scopes.selected\")` and set it if it is not, adding to the existing scopes.  \r\n4. Alternativly, set the `googleAuthR` option for Google Cloud storage scope after the library has been loaded but before authentication. \r\n\r\n        options(googleAuthR.scopes.selected = \"https://www.googleapis.com/auth/devstorage.full_control\")\r\n\r\n## Installation ##\r\n\r\n[![CRAN](http://www.r-pkg.org/badges/version/googleCloudStorageR)](http://cran.r-project.org/package=googleCloudStorageR)\r\n[![Build Status](https://travis-ci.org/cloudyr/googleCloudStorageR.png?branch=master)](https://travis-ci.org/cloudyr/googleCloudStorageR)\r\n[![codecov.io](http://codecov.io/github/cloudyr/googleCloudStorageR/coverage.svg?branch=master)](http://codecov.io/github/cloudyr/googleCloudStorageR?branch=master)\r\n\r\nThis package is on CRAN.\r\n\r\n```R\r\n# latest stable version\r\ninstall.packages(\"googleCloudStorageR\")\r\n```\r\n\r\nOr, to pull a potentially unstable version directly from GitHub:\r\n\r\n```R\r\nif(!require(\"ghit\")){\r\n    install.packages(\"ghit\")\r\n}\r\nghit::install_github(\"cloudyr/googleCloudStorageR\")\r\n```\r\n\r\n\r\n---\r\n[![cloudyr project logo](http://i.imgur.com/JHS98Y7.png)](https://github.com/cloudyr)\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}